# Preprocessing
Before training the model, the text data was preprocessed by cleaning it and removing stopwords, punctuations, and other irrelevant characters. The text was tokenized using the BERT tokenizer and truncated to a maximum length of 512 tokens. The labels were encoded as integers using a label encoder.
